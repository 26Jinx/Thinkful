{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Predict the running times of prospective Olympic sprinters using data from the last 20 Olympics.\n",
    "\n",
    "First thing we need to do is determine whether the desired prediction is a type of classification or regression. Since we are trying to predict the times of sprinters, we will need to employ a regression model to predict our outcomes. If we are to take the times of races from the past 20 Olympics, this wouldn't necessarily be a very feature-rich dataset so the best algorithm might be to use the K-Nearest Neighbors. The only problem we might run into is that we won't be able to predict any record-breaking times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. You have more features (columns) than rows in your dataset.\n",
    "\n",
    "Having more features than rows means either that we have a very small sample or that we have a very large set of features. In either case, linear regression will be the best bet because if we have a very small sample we have a simple and fast model that resists overfitting but is susceptible to outliers, and if we have a very large dataset with numerous features, we can handle the feature set with regularization by using ridge and lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Identify the most important characteristic predicting likelihood of being jailed before age 20.\n",
    "\n",
    "Since we want to predict the likelihood or the odds of someone being jailed before the age of 20, logistic regression would be the most straight forward choice. And it would be easy to gather the information we need by using the coefficients of the logistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement a filter to “highlight” emails that might be important to the recipient\n",
    "\n",
    "What we need here is a algorithm that is quick to predict (we need to classify as the email is recieved) and is able to classify the emails into multiple categories. Support Vector Machine should do well in this case since it handles classifying in multiple dimensions well and is fast in predicting new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. You have 1000+ features.\n",
    "\n",
    "With this many features, SVM might be the way to go since it scales well with large feature sets, but if explaining the feature importance is what we are after, we could be better off with using ridge or lasso regression that will be able to more easily explain the degress of feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Predict whether someone who adds items to their cart on a website will purchase the items.\n",
    "\n",
    "I think random forest is the most obvious choice here since we are actually trying to predict the outcome using decisions made by the user and speed isn't necessarily what we need in this case. Again, if we need to explain the important factors that equates to the user either making the purchase or not, we would need to use another classfication algorithm such as logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Your dataset dimensions are 982400 x 500\n",
    "\n",
    "A dataset of this magnitude calls for gradient boosting since it can handle large datasets, but it might take a little longer to train the data. It would be an excellent choice due to its accuracy and the tunable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Identify faces in an image.\n",
    "\n",
    "Identifying faces in an image certainly requires an algorithm that can handle non-linear classification. SVM is great at handling complex datasets and is quite fast as well. There probably isn't a great need to explain feature importance in this case, so SVM should work nicely. KNN also might be quite handy here as well if speed isn't a factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Predict which of three flavors of ice cream will be most popular with boys vs girls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are trying to predict the flavor of ice cream given that the subject is a boy (or a girl), Naive-Bayes might just be the tool here to classify predictions. It is simple to implement and easy to understand. I think KNN classifier could also work here but simpler option just might be the answer in this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
