{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "It's come time to address another potential source of error in our models: overfitting. __Overfitting__ is when your model is so excessively complex that it starts to catch random noise instead of describing the true underlying relationships. This is typically manifested with a model that evaluates as more accurate than it really is. In most situations you shouldn't be able to build a perfect model, some error is to be expected. Overfitting is extremely common and easy to do, but there are ways to guard against it. The main way is through how you evaluate your model.\n",
    "\n",
    "Thus far we've been using our training data to evaluate our model. By this we mean that we've used the same data to train the model and to see how well the model is doing. When you think about it, some of the danger of that approach may become apparent. If we create a very elaborate model it will pick up on the nuances of the data that are just from random noise.  If we evaluate the model on the training data, that ability to pick up noise will be returned as accuracy. In reality, this isn't the case and doesn't depict how we'd really want to evaluate a model. Generally we don't care about predicting things we already know. We care about other data, new information, or other situations. This is why testing with training data really isn't what we want to do. \n",
    "\n",
    "But if that's the case, what can we do?\n",
    "\n",
    "## Holdout Groups\n",
    "\n",
    "The simplest way to combat overfitting is with a **holdout group** (or sometimes \"holdback group\"). All this means is that you do not include all of your data in your training set, instead reserving some of it exclusively for testing. While there is a cost to having less training data, your evaluation will be far more reliable.\n",
    "\n",
    "When directly comparing two models that are based on different techniques or different specifications, this holdout method combats overfitting. Overfit models will see a drop in success rate outside of their training data, and so their performance will not be artificially inflated as it would be if you trained and validated your model using the whole data set. This is because they got really good at matching the patterns within the data they were trained with, but didn't actually learn the things that matter but random noise. When they try to match that random noise on new data their accuracy suffers.\n",
    "\n",
    "How much data you choose to keep in a holdout is really up to you and depends on how much and what kind of data you have to begin with as well as what kind of model you're training. You should check and see how much variance your model has as you add more data as well as how much data it would take to maintain a reasonably representative test sample. It is, however, a balance. 30% is a common starting point, but really anything from 50% to 1% of the original dataset could be reasonable.\n",
    "\n",
    "This seems relatively simple to code up. We'll try it below with our spam model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Grab and process the raw data.\n",
    "data_path = (\"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "             \"master/sms_spam_collection/SMSSpamCollection\"\n",
    "            )\n",
    "sms_raw = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
    "sms_raw.columns = ['spam', 'message']\n",
    "\n",
    "# Enumerate our spammy keywords.\n",
    "keywords = ['click', 'offer', 'winner', 'buy', 'free', 'cash', 'urgent']\n",
    "\n",
    "for key in keywords:\n",
    "    sms_raw[str(key)] = sms_raw.message.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    ")\n",
    "\n",
    "sms_raw['allcaps'] = sms_raw.message.str.isupper()\n",
    "sms_raw['spam'] = (sms_raw['spam'] == 'spam')\n",
    "data = sms_raw[keywords + ['allcaps']]\n",
    "target = sms_raw['spam']\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "y_pred = bnb.fit(data, target).predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20% Holdout: 0.884304932735426\n",
      "Testing on Sample: 0.8916008614501076\n"
     ]
    }
   ],
   "source": [
    "# Test your model with different holdout groups.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Use train_test_split to create the necessary training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores look really consistent! It doesn't seem like our model is overfitting. Part of the reason for that is that it's so simple (more on that in a bit). But we should look and see if any other issues are lurking here. So let's try a more robust evaluation technique, cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Cross Validation\n",
    "\n",
    "Cross validation is a more robust version of holdout groups. Instead of creating just one holdout, you create several.\n",
    "\n",
    "The way it works is this: start by breaking up your data into several equally sized pieces, or __folds__. Let's say you make _x_ folds. You then go through the training and testing process _x_ times, each time with a different fold held out from the training data and used as the test set. The number of folds you create is up to you, but it will depend on how much data you want in your testing set. At its most extreme, you're creating the same number of folds as you have observations in your data set. This kind of cross validation has a special name: __Leave One Out__. Leave one out is useful if you're worried about single observations skewing your model, whereas large folds combat more general overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89784946, 0.89426523, 0.89426523, 0.890681  , 0.89605735,\n",
       "       0.89048474, 0.88150808, 0.89028777, 0.88489209, 0.89568345])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's exactly what we'd hope to see. The array that `cross_val_score` returns is a series of accuracy scores with a different hold out group each time. If our model is overfitting at a variable amount, those scores will fluctuate. Instead, ours are relatively consistent.\n",
    "\n",
    "Above we used the SKLearn built in functions for both of these kinds of cross validation, the documentation for which can be found [here](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators-with-stratification-based-on-class-labels). However, the outputs from that are somewhat limited. By default it uses the `score` method. You can adjust what is returned, but you don't get all of the error types or outputs you may be interested in. That's why it's not uncommon for people to code up their own cross validation.\n",
    "\n",
    "To make sure you understand how cross validation works, try to code it up yourself below, not relying on SKLearn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8886894075403949,\n",
       " 0.8761220825852782,\n",
       " 0.9048473967684022,\n",
       " 0.895870736086176,\n",
       " 0.899641577060932,\n",
       " 0.8994614003590664,\n",
       " 0.8850987432675045,\n",
       " 0.8833034111310593,\n",
       " 0.8868940754039497,\n",
       " 0.8960573476702509]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement your own cross validation with your spam model.\n",
    "# Perform your additional evaluation here.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "holdout_data = []\n",
    "holdout_target = []\n",
    "training_data = []\n",
    "training_target = []\n",
    "x_val_score = []\n",
    "\n",
    "def x_validate(d, trg, folds):\n",
    "    size = len(d)/folds\n",
    "    current_index = 0\n",
    "    \n",
    "    # Iterate over each list and create hold out data\n",
    "    for i in range(folds):\n",
    "        \n",
    "        # Make copies of data passed in\n",
    "        training_data = d[:]\n",
    "        training_target = trg[:]\n",
    "        \n",
    "        # Create holdout data and target\n",
    "        holdout_data = training_data.iloc[current_index:int(size*(i+1))]        \n",
    "        holdout_target = training_target.iloc[current_index:int(size*(i+1))]\n",
    "        \n",
    "        # Drop test rows from training data and target\n",
    "        training_data = training_data.drop(training_data.index[current_index:int(size*(i+1))])\n",
    "        training_target = training_target.drop(training_target.index[current_index:int(size*(i+1))])\n",
    "        \n",
    "        current_index = int(size*(i+1))\n",
    "        \n",
    "        #print(len(holdout_data), len(training_data))\n",
    "        #print(bnb.fit(training_data, training_target).score(holdout_data, holdout_target))\n",
    "        x_val_score.append(bnb.fit(training_data, training_target).score(holdout_data, holdout_target))\n",
    "        \n",
    "    return x_val_score\n",
    "\n",
    "x_validate(data, target, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## What's a good score?\n",
    "\n",
    "When we're looking at this model, we've been getting accuracy scores around .89. Intuitively that seems like a pretty good score, but in the start of this lesson we mentioned different kinds of error. We also mentioned class imbalance. Both of these things are at play here. Using the topics we introduced earlier in this lesson, try to do a more in depth evaluation of the model looking at the kind of errors we're generating and what accuracy we'd get if we just randomly guessed. You may want to use what's known as a [confusion matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to show different kinds of errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[470   7]\n",
      " [ 55  25]]\n",
      "0      False\n",
      "1      False\n",
      "2       True\n",
      "3      False\n",
      "4      False\n",
      "5       True\n",
      "6      False\n",
      "7      False\n",
      "8       True\n",
      "9       True\n",
      "10     False\n",
      "11      True\n",
      "12      True\n",
      "13     False\n",
      "14     False\n",
      "15      True\n",
      "16     False\n",
      "17     False\n",
      "18     False\n",
      "19      True\n",
      "20     False\n",
      "21     False\n",
      "22     False\n",
      "23     False\n",
      "24     False\n",
      "25     False\n",
      "26     False\n",
      "27     False\n",
      "28     False\n",
      "29     False\n",
      "       ...  \n",
      "527     True\n",
      "528    False\n",
      "529     True\n",
      "530    False\n",
      "531     True\n",
      "532    False\n",
      "533    False\n",
      "534    False\n",
      "535    False\n",
      "536    False\n",
      "537    False\n",
      "538    False\n",
      "539    False\n",
      "540    False\n",
      "541     True\n",
      "542    False\n",
      "543    False\n",
      "544    False\n",
      "545    False\n",
      "546    False\n",
      "547    False\n",
      "548    False\n",
      "549    False\n",
      "550    False\n",
      "551    False\n",
      "552    False\n",
      "553    False\n",
      "554    False\n",
      "555    False\n",
      "556    False\n",
      "Name: spam, Length: 557, dtype: bool\n",
      "Sensitivity: 0.3125\n",
      " Specificity: 0.9853249475890985\n",
      "[[464   5]\n",
      " [ 64  24]]\n",
      "557     False\n",
      "558     False\n",
      "559     False\n",
      "560     False\n",
      "561     False\n",
      "562     False\n",
      "563     False\n",
      "564      True\n",
      "565     False\n",
      "566     False\n",
      "567     False\n",
      "568     False\n",
      "569     False\n",
      "570     False\n",
      "571     False\n",
      "572     False\n",
      "573     False\n",
      "574     False\n",
      "575     False\n",
      "576      True\n",
      "577     False\n",
      "578     False\n",
      "579      True\n",
      "580     False\n",
      "581     False\n",
      "582     False\n",
      "583      True\n",
      "584     False\n",
      "585     False\n",
      "586     False\n",
      "        ...  \n",
      "1084    False\n",
      "1085    False\n",
      "1086    False\n",
      "1087    False\n",
      "1088    False\n",
      "1089     True\n",
      "1090    False\n",
      "1091     True\n",
      "1092    False\n",
      "1093    False\n",
      "1094    False\n",
      "1095    False\n",
      "1096    False\n",
      "1097     True\n",
      "1098    False\n",
      "1099    False\n",
      "1100    False\n",
      "1101    False\n",
      "1102    False\n",
      "1103    False\n",
      "1104    False\n",
      "1105     True\n",
      "1106    False\n",
      "1107    False\n",
      "1108    False\n",
      "1109    False\n",
      "1110    False\n",
      "1111    False\n",
      "1112    False\n",
      "1113    False\n",
      "Name: spam, Length: 557, dtype: bool\n",
      "Sensitivity: 0.2727272727272727\n",
      " Specificity: 0.9893390191897654\n",
      "[[484   4]\n",
      " [ 49  20]]\n",
      "1114    False\n",
      "1115    False\n",
      "1116    False\n",
      "1117    False\n",
      "1118     True\n",
      "1119    False\n",
      "1120     True\n",
      "1121    False\n",
      "1122     True\n",
      "1123    False\n",
      "1124    False\n",
      "1125    False\n",
      "1126     True\n",
      "1127    False\n",
      "1128    False\n",
      "1129     True\n",
      "1130    False\n",
      "1131    False\n",
      "1132    False\n",
      "1133    False\n",
      "1134    False\n",
      "1135    False\n",
      "1136    False\n",
      "1137     True\n",
      "1138    False\n",
      "1139    False\n",
      "1140    False\n",
      "1141    False\n",
      "1142     True\n",
      "1143    False\n",
      "        ...  \n",
      "1641    False\n",
      "1642    False\n",
      "1643    False\n",
      "1644    False\n",
      "1645    False\n",
      "1646    False\n",
      "1647    False\n",
      "1648    False\n",
      "1649    False\n",
      "1650    False\n",
      "1651    False\n",
      "1652    False\n",
      "1653     True\n",
      "1654    False\n",
      "1655    False\n",
      "1656    False\n",
      "1657    False\n",
      "1658    False\n",
      "1659     True\n",
      "1660    False\n",
      "1661    False\n",
      "1662    False\n",
      "1663     True\n",
      "1664    False\n",
      "1665    False\n",
      "1666    False\n",
      "1667    False\n",
      "1668    False\n",
      "1669    False\n",
      "1670    False\n",
      "Name: spam, Length: 557, dtype: bool\n",
      "Sensitivity: 0.2898550724637681\n",
      " Specificity: 0.9918032786885246\n",
      "[[481   4]\n",
      " [ 54  18]]\n",
      "1671    False\n",
      "1672    False\n",
      "1673     True\n",
      "1674     True\n",
      "1675    False\n",
      "1676    False\n",
      "1677    False\n",
      "1678    False\n",
      "1679    False\n",
      "1680    False\n",
      "1681    False\n",
      "1682    False\n",
      "1683    False\n",
      "1684    False\n",
      "1685    False\n",
      "1686    False\n",
      "1687     True\n",
      "1688     True\n",
      "1689    False\n",
      "1690    False\n",
      "1691     True\n",
      "1692    False\n",
      "1693    False\n",
      "1694    False\n",
      "1695    False\n",
      "1696    False\n",
      "1697    False\n",
      "1698    False\n",
      "1699     True\n",
      "1700    False\n",
      "        ...  \n",
      "2198    False\n",
      "2199    False\n",
      "2200    False\n",
      "2201    False\n",
      "2202    False\n",
      "2203    False\n",
      "2204    False\n",
      "2205    False\n",
      "2206    False\n",
      "2207     True\n",
      "2208    False\n",
      "2209     True\n",
      "2210    False\n",
      "2211    False\n",
      "2212    False\n",
      "2213    False\n",
      "2214    False\n",
      "2215    False\n",
      "2216    False\n",
      "2217    False\n",
      "2218    False\n",
      "2219    False\n",
      "2220     True\n",
      "2221    False\n",
      "2222    False\n",
      "2223    False\n",
      "2224     True\n",
      "2225    False\n",
      "2226    False\n",
      "2227    False\n",
      "Name: spam, Length: 557, dtype: bool\n",
      "Sensitivity: 0.25\n",
      " Specificity: 0.9917525773195877\n",
      "[[481   5]\n",
      " [ 51  21]]\n",
      "2228    False\n",
      "2229    False\n",
      "2230    False\n",
      "2231    False\n",
      "2232    False\n",
      "2233    False\n",
      "2234    False\n",
      "2235    False\n",
      "2236    False\n",
      "2237    False\n",
      "2238    False\n",
      "2239    False\n",
      "2240    False\n",
      "2241    False\n",
      "2242    False\n",
      "2243    False\n",
      "2244    False\n",
      "2245    False\n",
      "2246    False\n",
      "2247     True\n",
      "2248     True\n",
      "2249    False\n",
      "2250     True\n",
      "2251    False\n",
      "2252    False\n",
      "2253    False\n",
      "2254    False\n",
      "2255    False\n",
      "2256    False\n",
      "2257    False\n",
      "        ...  \n",
      "2756    False\n",
      "2757    False\n",
      "2758    False\n",
      "2759    False\n",
      "2760    False\n",
      "2761    False\n",
      "2762    False\n",
      "2763    False\n",
      "2764    False\n",
      "2765    False\n",
      "2766    False\n",
      "2767     True\n",
      "2768    False\n",
      "2769    False\n",
      "2770     True\n",
      "2771    False\n",
      "2772    False\n",
      "2773    False\n",
      "2774     True\n",
      "2775    False\n",
      "2776    False\n",
      "2777    False\n",
      "2778    False\n",
      "2779     True\n",
      "2780    False\n",
      "2781    False\n",
      "2782    False\n",
      "2783    False\n",
      "2784    False\n",
      "2785    False\n",
      "Name: spam, Length: 558, dtype: bool\n",
      "Sensitivity: 0.2916666666666667\n",
      " Specificity: 0.9897119341563786\n",
      "[[488   4]\n",
      " [ 52  13]]\n",
      "2786    False\n",
      "2787    False\n",
      "2788    False\n",
      "2789    False\n",
      "2790    False\n",
      "2791     True\n",
      "2792    False\n",
      "2793    False\n",
      "2794    False\n",
      "2795    False\n",
      "2796    False\n",
      "2797    False\n",
      "2798    False\n",
      "2799    False\n",
      "2800    False\n",
      "2801    False\n",
      "2802    False\n",
      "2803    False\n",
      "2804     True\n",
      "2805    False\n",
      "2806    False\n",
      "2807    False\n",
      "2808     True\n",
      "2809    False\n",
      "2810    False\n",
      "2811    False\n",
      "2812    False\n",
      "2813    False\n",
      "2814    False\n",
      "2815    False\n",
      "        ...  \n",
      "3313    False\n",
      "3314    False\n",
      "3315    False\n",
      "3316     True\n",
      "3317    False\n",
      "3318    False\n",
      "3319    False\n",
      "3320    False\n",
      "3321    False\n",
      "3322    False\n",
      "3323    False\n",
      "3324    False\n",
      "3325    False\n",
      "3326    False\n",
      "3327    False\n",
      "3328    False\n",
      "3329    False\n",
      "3330    False\n",
      "3331    False\n",
      "3332    False\n",
      "3333    False\n",
      "3334     True\n",
      "3335    False\n",
      "3336    False\n",
      "3337    False\n",
      "3338    False\n",
      "3339    False\n",
      "3340    False\n",
      "3341    False\n",
      "3342    False\n",
      "Name: spam, Length: 557, dtype: bool\n",
      "Sensitivity: 0.2\n",
      " Specificity: 0.991869918699187\n",
      "[[477   7]\n",
      " [ 57  16]]\n",
      "3343    False\n",
      "3344    False\n",
      "3345    False\n",
      "3346    False\n",
      "3347    False\n",
      "3348    False\n",
      "3349    False\n",
      "3350    False\n",
      "3351    False\n",
      "3352    False\n",
      "3353    False\n",
      "3354    False\n",
      "3355    False\n",
      "3356    False\n",
      "3357    False\n",
      "3358    False\n",
      "3359    False\n",
      "3360     True\n",
      "3361    False\n",
      "3362    False\n",
      "3363    False\n",
      "3364    False\n",
      "3365    False\n",
      "3366    False\n",
      "3367    False\n",
      "3368    False\n",
      "3369    False\n",
      "3370    False\n",
      "3371    False\n",
      "3372    False\n",
      "        ...  \n",
      "3870    False\n",
      "3871    False\n",
      "3872    False\n",
      "3873    False\n",
      "3874    False\n",
      "3875    False\n",
      "3876    False\n",
      "3877    False\n",
      "3878    False\n",
      "3879    False\n",
      "3880    False\n",
      "3881    False\n",
      "3882    False\n",
      "3883    False\n",
      "3884    False\n",
      "3885     True\n",
      "3886    False\n",
      "3887    False\n",
      "3888    False\n",
      "3889    False\n",
      "3890    False\n",
      "3891     True\n",
      "3892    False\n",
      "3893     True\n",
      "3894    False\n",
      "3895     True\n",
      "3896    False\n",
      "3897     True\n",
      "3898    False\n",
      "3899    False\n",
      "Name: spam, Length: 557, dtype: bool\n",
      "Sensitivity: 0.2191780821917808\n",
      " Specificity: 0.9855371900826446\n",
      "[[470   4]\n",
      " [ 61  22]]\n",
      "3900    False\n",
      "3901    False\n",
      "3902    False\n",
      "3903    False\n",
      "3904    False\n",
      "3905     True\n",
      "3906     True\n",
      "3907    False\n",
      "3908    False\n",
      "3909    False\n",
      "3910    False\n",
      "3911    False\n",
      "3912    False\n",
      "3913     True\n",
      "3914    False\n",
      "3915    False\n",
      "3916    False\n",
      "3917    False\n",
      "3918    False\n",
      "3919    False\n",
      "3920    False\n",
      "3921     True\n",
      "3922    False\n",
      "3923    False\n",
      "3924    False\n",
      "3925    False\n",
      "3926    False\n",
      "3927    False\n",
      "3928    False\n",
      "3929    False\n",
      "        ...  \n",
      "4427    False\n",
      "4428    False\n",
      "4429    False\n",
      "4430    False\n",
      "4431    False\n",
      "4432    False\n",
      "4433    False\n",
      "4434    False\n",
      "4435    False\n",
      "4436     True\n",
      "4437    False\n",
      "4438    False\n",
      "4439    False\n",
      "4440    False\n",
      "4441    False\n",
      "4442    False\n",
      "4443    False\n",
      "4444    False\n",
      "4445    False\n",
      "4446    False\n",
      "4447    False\n",
      "4448    False\n",
      "4449    False\n",
      "4450     True\n",
      "4451    False\n",
      "4452    False\n",
      "4453    False\n",
      "4454    False\n",
      "4455    False\n",
      "4456    False\n",
      "Name: spam, Length: 557, dtype: bool\n",
      "Sensitivity: 0.26506024096385544\n",
      " Specificity: 0.9915611814345991\n",
      "[[477   7]\n",
      " [ 56  17]]\n",
      "4457    False\n",
      "4458    False\n",
      "4459    False\n",
      "4460     True\n",
      "4461    False\n",
      "4462    False\n",
      "4463    False\n",
      "4464    False\n",
      "4465    False\n",
      "4466    False\n",
      "4467    False\n",
      "4468    False\n",
      "4469    False\n",
      "4470    False\n",
      "4471    False\n",
      "4472    False\n",
      "4473     True\n",
      "4474    False\n",
      "4475     True\n",
      "4476    False\n",
      "4477    False\n",
      "4478    False\n",
      "4479    False\n",
      "4480    False\n",
      "4481    False\n",
      "4482    False\n",
      "4483    False\n",
      "4484    False\n",
      "4485    False\n",
      "4486    False\n",
      "        ...  \n",
      "4984    False\n",
      "4985     True\n",
      "4986    False\n",
      "4987    False\n",
      "4988    False\n",
      "4989    False\n",
      "4990    False\n",
      "4991     True\n",
      "4992    False\n",
      "4993    False\n",
      "4994    False\n",
      "4995    False\n",
      "4996    False\n",
      "4997    False\n",
      "4998    False\n",
      "4999    False\n",
      "5000    False\n",
      "5001    False\n",
      "5002    False\n",
      "5003    False\n",
      "5004     True\n",
      "5005    False\n",
      "5006    False\n",
      "5007    False\n",
      "5008    False\n",
      "5009    False\n",
      "5010    False\n",
      "5011    False\n",
      "5012     True\n",
      "5013    False\n",
      "Name: spam, Length: 557, dtype: bool\n",
      "Sensitivity: 0.2328767123287671\n",
      " Specificity: 0.9855371900826446\n",
      "[[478   8]\n",
      " [ 50  22]]\n",
      "5014    False\n",
      "5015    False\n",
      "5016    False\n",
      "5017    False\n",
      "5018     True\n",
      "5019    False\n",
      "5020    False\n",
      "5021    False\n",
      "5022    False\n",
      "5023    False\n",
      "5024    False\n",
      "5025    False\n",
      "5026    False\n",
      "5027     True\n",
      "5028     True\n",
      "5029    False\n",
      "5030     True\n",
      "5031    False\n",
      "5032    False\n",
      "5033    False\n",
      "5034    False\n",
      "5035    False\n",
      "5036    False\n",
      "5037     True\n",
      "5038    False\n",
      "5039    False\n",
      "5040    False\n",
      "5041     True\n",
      "5042    False\n",
      "5043     True\n",
      "        ...  \n",
      "5542    False\n",
      "5543    False\n",
      "5544    False\n",
      "5545    False\n",
      "5546    False\n",
      "5547     True\n",
      "5548    False\n",
      "5549    False\n",
      "5550    False\n",
      "5551    False\n",
      "5552    False\n",
      "5553    False\n",
      "5554    False\n",
      "5555    False\n",
      "5556    False\n",
      "5557    False\n",
      "5558    False\n",
      "5559    False\n",
      "5560    False\n",
      "5561    False\n",
      "5562    False\n",
      "5563    False\n",
      "5564    False\n",
      "5565    False\n",
      "5566     True\n",
      "5567     True\n",
      "5568    False\n",
      "5569    False\n",
      "5570    False\n",
      "5571    False\n",
      "Name: spam, Length: 558, dtype: bool\n",
      "Sensitivity: 0.3055555555555556\n",
      " Specificity: 0.9835390946502057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8886894075403949,\n",
       " 0.8761220825852782,\n",
       " 0.9048473967684022,\n",
       " 0.895870736086176,\n",
       " 0.899641577060932,\n",
       " 0.8994614003590664,\n",
       " 0.8850987432675045,\n",
       " 0.8833034111310593,\n",
       " 0.8868940754039497,\n",
       " 0.8960573476702509,\n",
       " 0.8886894075403949,\n",
       " 0.8761220825852782,\n",
       " 0.9048473967684022,\n",
       " 0.895870736086176,\n",
       " 0.899641577060932,\n",
       " 0.8994614003590664,\n",
       " 0.8850987432675045,\n",
       " 0.8833034111310593,\n",
       " 0.8868940754039497,\n",
       " 0.8960573476702509,\n",
       " 0.8886894075403949,\n",
       " 0.8761220825852782,\n",
       " 0.9048473967684022,\n",
       " 0.895870736086176,\n",
       " 0.899641577060932,\n",
       " 0.8994614003590664,\n",
       " 0.8850987432675045,\n",
       " 0.8833034111310593,\n",
       " 0.8868940754039497,\n",
       " 0.8960573476702509,\n",
       " 0.8886894075403949,\n",
       " 0.8761220825852782,\n",
       " 0.9048473967684022,\n",
       " 0.895870736086176,\n",
       " 0.899641577060932,\n",
       " 0.8994614003590664,\n",
       " 0.8850987432675045,\n",
       " 0.8833034111310593,\n",
       " 0.8868940754039497,\n",
       " 0.8960573476702509]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform your additional evaluation here.\n",
    "def x_validate(d, trg, folds):\n",
    "    size = len(d)/folds\n",
    "    current_index = 0\n",
    "    \n",
    "    # Iterate over each list and create hold out data\n",
    "    for i in range(folds):\n",
    "        \n",
    "        # Make copies of data passed in\n",
    "        training_data = d[:]\n",
    "        training_target = trg[:]\n",
    "        \n",
    "        # Create holdout data and target\n",
    "        holdout_data = training_data.iloc[current_index:int(size*(i+1))]        \n",
    "        holdout_target = training_target.iloc[current_index:int(size*(i+1))]\n",
    "        \n",
    "        # Drop test rows from training data and target\n",
    "        training_data = training_data.drop(training_data.index[current_index:int(size*(i+1))])\n",
    "        training_target = training_target.drop(training_target.index[current_index:int(size*(i+1))])\n",
    "        \n",
    "        current_index = int(size*(i+1))\n",
    "        \n",
    "        #print(len(holdout_data), len(training_data))\n",
    "        #print(bnb.fit(training_data, training_target).score(holdout_data, holdout_target))\n",
    "        x_val_score.append(bnb.fit(training_data, training_target).score(holdout_data, holdout_target))\n",
    "        y_pred = bnb.fit(training_data, training_target).predict(holdout_data)\n",
    "        c_mat = confusion_matrix(holdout_target, y_pred)\n",
    "        print(c_mat)\n",
    "        print(holdout_target)\n",
    "        \n",
    "        # Sensitivity (Rate of correct prediction for spam)\n",
    "        # True-pos / sum(True-pos & False-neg)\n",
    "        sens = c_mat[1, 1]/sum(c_mat[1])\n",
    "        \n",
    "        # Specificity (Rate of correct prediction for ham)\n",
    "        # True-neg / sum(True-neg & False-pos)\n",
    "        spec = c_mat[0, 0]/sum(c_mat[0])\n",
    "        \n",
    "        print(\"Sensitivity: \" + str(sens) + '\\n',\n",
    "              \"Specificity: \" + str(spec))\n",
    "        \n",
    "    return x_val_score\n",
    "\n",
    "x_validate(data, target, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8655914 , 0.8655914 , 0.8655914 , 0.8655914 , 0.8655914 ,\n",
       "       0.86535009, 0.86535009, 0.86690647, 0.86690647, 0.86690647])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random guesses\n",
    "random_data = pd.DataFrame(np.random.randint(0, 1, size=(data.shape[0], data.shape[1])))\n",
    "random_data.head()\n",
    "\n",
    "cross_val_score(bnb, random_data, target, cv=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like even with random data, we get around 86% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "\n",
    "## Thinking like a Data Scientist\n",
    "\n",
    "How you choose to validate your model in real life will depend upon the kind of data you're working with and the kinds of concerns you have about the model's performance. Remember, your model is trained to fit the data you feed it, so if the situation changes your model will become less accurate. For example, if there are seasonal changes to your observed variable but you only train on one month's data, you're going to have a problem. You could test that by seeing how accurate your model is with a specific time period as your holdout, rather than a random sample. We'll cover techniques for dealing with time more later.\n",
    "\n",
    "## Overfitting and Naive Bayes\n",
    "\n",
    "Overfitting is always possible, but some models are more susceptible to it than others. Naive Bayes is actually pretty good for avoiding overfitting. This is largely because the assumptions are so simple, particularly the assumed independence between any two independent variables. One of the sources of overfitting is when a model tries to map complex interactions between variables that aren't really there or significant. Naive Bayes cannot do this because it assumes they are all independent and therefore not interacting. It's a nice characteristic at times, but it does mean it doesn't take into account how your features affect each other.\n",
    "\n",
    "Also, one final note on our models here. They weren't overfitting, but they weren't telling us much either. They were just barely more accurate than the dominant class. Discuss with your mentor why that is and what you could do to improve the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
