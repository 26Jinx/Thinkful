{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Spark Streaming with network traffic data\n",
    "\n",
    "Now that we have worked through some examples with batch data, it's time to turn our attention to Spark's Structured Streaming. Our end goal today is to stream and process data in real time with Spark. There are a ton of streaming data sources out there (think Foursquare, Twitter streaming, Citibike data, NYC MTA train arrivals, and so on). Those are all excellent data sources, and you'll get an opportunity to work with Foursquare's api in the next assignment. But as you're first learning about streaming, there's a problem with using real time data — namely, how can we really know that we've got our data right? What we need is a static baseline to compare our streaming analysis against to ensure that we get what is arguably the hardest part of streaming right: properly formatting incoming data and ensuring data integrity.\n",
    "\n",
    "To that end, we actually won't do much heavy lifting on the machine learning side of things. Our analysis will be relatively simple. We'll explore streaming data by working with a small subset of the Los Alamos National Lab network dataset, which contains data about Internet traffic. First we'll do a static analysis of this data to get a baseline result, and then you'll re-run the analysis by streaming that same data on Colab. This will prepare you for working with real time streaming data in the next assignment.\n",
    "\n",
    "Like with the introduction to batch processing, we'll give you the background context you need about the data and approach here, and then you'll go through a walkthrough in a Jupyter notebook we provide in your big data student resources directory.\n",
    "\n",
    "## Background on streaming\n",
    "\n",
    "Before you start, it's a good idea to read through the following articles:\n",
    "\n",
    "* [Structured Streaming in Apache Spark](https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html)\n",
    "* [Spark Streaming Programming Guide](https://spark.apache.org/docs/latest/streaming-programming-guide.html)\n",
    "\n",
    "These resources will give you a sense of how one can use streaming - where we can see an advantage, as well as some of the challenges we'll face. Make no mistake, either —— streaming can be hard to get right. In fact, we'll warn you up front that this walkthrough is challenging. But, it's worth it!\n",
    "\n",
    "## The Los Alamos National Lab network dataset\n",
    "\n",
    "Los Alamos has posted a huge amount of network data online in CSV files (available [here](https://csr.lanl.gov/data/2017.html)). For the purposes of this walk-through, the original data set was way too large. To make things manageable, we took a small subset of the data - only about 2.5 minutes worth, comprising around 500,000 flows — and split it up into a series of 50 JSON files because JSON is the most common format for streaming data. This will allow us to simulate streaming real time data by treating each of the 50 JSON files as a separate stream object. Each entry in the dataset represents a \"conversation\" between 2 computers. For each entry, the following fields are captured:\n",
    "\n",
    "* Time: the start time of the conversation (in a proprietary timestamp format)\n",
    "\n",
    "* Duration: the length of the conversation (in seconds)\n",
    "\n",
    "* SrcDevice: name of the device that initiated the conversation\n",
    "\n",
    "* DstDevice: name of the device that was requested\n",
    "\n",
    "* Protocol: network protocol used (TCP, UDP, etc)\n",
    "\n",
    "* SrcPort: network port (0-65,536) on the originating device\n",
    "\n",
    "* DstPort: network port (0-65,536) on the destination device\n",
    "\n",
    "* SrcPackets: network packet count sent from the source to the destination\n",
    "\n",
    "* DstPackets: network packet count sent from the destination to the source\n",
    "\n",
    "* SrcBytes: byte count sent from the source to the destination\n",
    "\n",
    "* DstBytes: byte count sent from the destination to the source \n",
    "\n",
    "As you can see, there's a lot here. However, we'll try to answer a relatively simple question: From this data, how many web servers can be identified?\n",
    "\n",
    "## Identifying web servers\n",
    "\n",
    "To answer our research question, we'll use some basic knowledge about network behavior. Web servers typically communicate on one of two ports: **8000 or **443**. So, if a computer requests one of those ports as the `dstPort` in a flow, it's likely that the destination device (`dstDevice`) is a web server. If we see that computer name come up repeatedly in our request list, then there's a good chance that device is a web server. So for our streaming exercise, we need to build a count query that processes streams as they come in and updates the count of web servers, then reports back to us what it sees.\n",
    "\n",
    "## Do the walkthrough\n",
    "\n",
    "With that background out of the way, it's time to work through the `Spark_streaming` notebook. As in the batch processing walkthrough before, you'll need to run this notebook on Colab. To this end, you need to upload your notebook to the `Colab Notebooks` folder. You also need to upload the data files directory (`datasets/lanl`) in student repository to the `Colab Datasets` folder in your drive. Since there are 50 files in the `datasets/lanl`, it would be easier to upload the folder to the `Colab Datasets` instead of uploading each file in the `lanl` folder one by one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
