{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [8, year, life, Galileo, house, arrest, espous...\n",
      "1    [2, 1912, olympian, football, star, Carlisle, ...\n",
      "2    [city, Yuma, state, record, average, 4,055, ho...\n",
      "3    [1963, live, Art, Linkletter, company, serve, ...\n",
      "4    [signer, December, Indep, framer, Constitution...\n",
      "5    [title, Aesop, fable, insect, share, billing, ...\n",
      "6    [build, 312, B.C., link, Rome, South, Italy, u...\n",
      "7    [8, 30, steal, Birmingham, Barons, 2,306, stea...\n",
      "8    [winter, 1971, 72, record, 1,122, inch, snow, ...\n",
      "9    [houseware, store, name, packaging, merchandis...\n",
      "dtype: object\n",
      "1684334\n",
      "['city', 'play', 'name', 'country', 'man', 'call', '2', 'know', 'see', 'like', 'type', 'film', 'say', 'state', 'U.S.', 'year', 'title', 'write', 'word', 'mean', 'win', 'come', 'include', 'large', 'bear', 'novel', 'find', 'term', 'New', 'time', 'star', 'work', '3', 'capital', 'president', '1', 'book', 'get', 'woman', 'go', 'old', 'take', 'famous', 'hit', 'song', 'day', 'world', 'John', 'give', 'home', 'begin', 'group', 'character', 'island', 'long', 'author', 'american', 'good', 'company', 'tell', 'people', 'century', 'end', 'tv', 'high', 'lead', 'movie', 'form', 'line', 'french', 'game', 'die', 'love', 'life', 'feature', '4', 'base', 'live', 'use', 'great', 'small', 'king', 'big', 'help', 'place', 'set', 'run', 'british', 'found', 'new', 'serve', 'water', 'hold', 'family', 'head', 'leave', 'body', 'number', 'war', 'animal']\n",
      "True\n",
      "done\n",
      "0  done\n",
      "25000  done\n",
      "50000  done\n",
      "75000  done\n",
      "100000  done\n",
      "125000  done\n",
      "150000  done\n",
      "175000  done\n",
      "200000  done\n",
      "[\"For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory\", 'No. 2: 1912 Olympian; football star at Carlisle Indian School; 6 MLB seasons with the Reds, Giants & Braves']\n",
      "Number of features: 27804\n",
      "151851\n",
      "Original sentence: A key element in local gov't is this type of meeting held annually on the first Tuesday in March\n",
      "Tf_idf vector: {'tuesday': 0.38064595263156303, 'annually': 0.3814241629024681, 'gov': 0.34760129702960896, 'meeting': 0.32618596809385936, 'local': 0.33576236297388967, 'held': 0.2726629347271754, 'march': 0.266798302350671, 'key': 0.3230567987505673, 'element': 0.2836734589934539, 'type': 0.19824534840456}\n",
      "(151851, 2000) (65079, 2000) (151851,) (65079,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5250889  0.52258659 0.52848205 0.52617715 0.52156734 0.52051366\n",
      " 0.52420151 0.5217649  0.52064537 0.52522392]\n",
      "0.9442611507332846\n",
      "0.5230719586963537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.54458053 0.54925589 0.55146526 0.55370431 0.54902865 0.55284821\n",
      " 0.55100428 0.55607507 0.54033586 0.54985511]\n",
      "Train score:  0.5763149403033236\n",
      "Test score:  0.5503618678836492\n",
      "[0.54892664 0.5508363  0.55278235 0.54863352 0.54626276 0.54784327\n",
      " 0.55745802 0.54527494 0.54395785 0.55031612]\n",
      "Train score:  0.5689195329632337\n",
      "Test score:  0.550576991041657\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "%run Challenge.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jeopardy! Categories!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the challenge exercise, I attempted to predict in which Jeopardy round a question might appear. It wasn't an overwhelming success as I was only able to get about 55% success rate in predictions. For this project, I am going to use clustering to see how ~28,000 categories can be grouped more broadly. I want to see if adding the category as a feature will improve the accuracy of my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>(For, the, last, 8, years, of, his, life, ,, G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>(No, ., 2, :, 1912, Olympian, ;, football, sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>(The, city, of, Yuma, in, this, state, has, a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>(In, 1963, ,, live, on, \", The, Art, Linklette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>(Signer, of, the, Dec., of, Indep, ., ,, frame...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Round                         Category  Value  \\\n",
       "0  Jeopardy!                          HISTORY   $200   \n",
       "1  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
       "2  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
       "3  Jeopardy!                 THE COMPANY LINE   $200   \n",
       "4  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
       "\n",
       "                                            Question  \\\n",
       "0  For the last 8 years of his life, Galileo was ...   \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...   \n",
       "2  The city of Yuma in this state has a record av...   \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...   \n",
       "4  Signer of the Dec. of Indep., framer of the Co...   \n",
       "\n",
       "                                             cleaned  \\\n",
       "0  For the last 8 years of his life, Galileo was ...   \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...   \n",
       "2  The city of Yuma in this state has a record av...   \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...   \n",
       "4  Signer of the Dec. of Indep., framer of the Co...   \n",
       "\n",
       "                                               spacy  \n",
       "0  (For, the, last, 8, years, of, his, life, ,, G...  \n",
       "1  (No, ., 2, :, 1912, Olympian, ;, football, sta...  \n",
       "2  (The, city, of, Yuma, in, this, state, has, a,...  \n",
       "3  (In, 1963, ,, live, on, \", The, Art, Linklette...  \n",
       "4  (Signer, of, the, Dec., of, Indep, ., ,, frame...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first let's take a closer look at the categories\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count             216930\n",
       "unique             27995\n",
       "top       BEFORE & AFTER\n",
       "freq                 547\n",
       "Name:  Category, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df[' Category'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BEFORE & AFTER               547\n",
       "SCIENCE                      519\n",
       "LITERATURE                   496\n",
       "AMERICAN HISTORY             418\n",
       "POTPOURRI                    401\n",
       "WORLD HISTORY                377\n",
       "WORD ORIGINS                 371\n",
       "COLLEGES & UNIVERSITIES      351\n",
       "HISTORY                      349\n",
       "SPORTS                       342\n",
       "U.S. CITIES                  339\n",
       "WORLD GEOGRAPHY              338\n",
       "BODIES OF WATER              327\n",
       "ANIMALS                      324\n",
       "STATE CAPITALS               314\n",
       "BUSINESS & INDUSTRY          311\n",
       "ISLANDS                      301\n",
       "WORLD CAPITALS               300\n",
       "U.S. GEOGRAPHY               299\n",
       "RELIGION                     297\n",
       "OPERA                        294\n",
       "SHAKESPEARE                  294\n",
       "LANGUAGES                    284\n",
       "BALLET                       282\n",
       "TELEVISION                   281\n",
       "FICTIONAL CHARACTERS         280\n",
       "RHYME TIME                   279\n",
       "TRANSPORTATION               279\n",
       "PEOPLE                       279\n",
       "STUPID ANSWERS               270\n",
       "                            ... \n",
       "BIBLICAL JOURNEYS              1\n",
       "TOP ATHLETES                   1\n",
       "OSCAR-WINNING SINGERS          1\n",
       "MODERN MATERIALS               1\n",
       "TECHNOLOGY FIRSTS              1\n",
       "MAP READING                    1\n",
       "THE PRESIDENTIAL CABINET       1\n",
       "GEM LORE                       1\n",
       "OLD NAMES IN THE NEWS          1\n",
       "SAME LAST NAMES                1\n",
       "FAMOUS OBJECTS                 1\n",
       "IRISH-BORN AUTHORS             1\n",
       "TRAVELING THE GLOBE            1\n",
       "POLITICAL JARGON               1\n",
       "AMERICAN WOMEN AUTHORS         1\n",
       "PSYCHOLOGICAL TERMS            1\n",
       "FAMOUS NAMES OF THE 1950s      1\n",
       "CABINET OFFICERS               1\n",
       "16th CENTURY EXPLORERS         1\n",
       "BRAND NAME PEOPLE              1\n",
       "AMERICAN PUBLISHING            1\n",
       "BOTANICAL ETYMOLOGY            1\n",
       "MEDICAL TERMINOLOGY            1\n",
       "IN THE NEWS 1952               1\n",
       "...SEE WHAT'S ON THE SLAB      1\n",
       "'90s OLYMPIC NAMES             1\n",
       "ISLAND CHAINS                  1\n",
       "AMERICAN FICTION WRITERS       1\n",
       "FAMILIAR PHRASE ORIGINS        1\n",
       "MILITARY NEWS                  1\n",
       "Name:  Category, Length: 27995, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df[' Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see right away that trying to cluster with just the category names might prove to be too difficult/inaccurate since the categories only consist of 2-3 words. Using tf-idf relies on frequency in the sentence compared with frequency in the document, so instead, I will combine the question and the category and then attempt to form clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>spacy</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>(For, the, last, 8, years, of, his, life, ,, G...</td>\n",
       "      <td>HISTORY For the last 8 years of his life, Gali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>(No, ., 2, :, 1912, Olympian, ;, football, sta...</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES No. 2: 1912 Ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>(The, city, of, Yuma, in, this, state, has, a,...</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT... The city of Yuma i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>(In, 1963, ,, live, on, \", The, Art, Linklette...</td>\n",
       "      <td>THE COMPANY LINE In 1963, live on \"The Art Lin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>(Signer, of, the, Dec., of, Indep, ., ,, frame...</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES Signer of the Dec. of Inde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Round                         Category  Value  \\\n",
       "0  Jeopardy!                          HISTORY   $200   \n",
       "1  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
       "2  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
       "3  Jeopardy!                 THE COMPANY LINE   $200   \n",
       "4  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
       "\n",
       "                                            Question  \\\n",
       "0  For the last 8 years of his life, Galileo was ...   \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...   \n",
       "2  The city of Yuma in this state has a record av...   \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...   \n",
       "4  Signer of the Dec. of Indep., framer of the Co...   \n",
       "\n",
       "                                             cleaned  \\\n",
       "0  For the last 8 years of his life, Galileo was ...   \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...   \n",
       "2  The city of Yuma in this state has a record av...   \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...   \n",
       "4  Signer of the Dec. of Indep., framer of the Co...   \n",
       "\n",
       "                                               spacy  \\\n",
       "0  (For, the, last, 8, years, of, his, life, ,, G...   \n",
       "1  (No, ., 2, :, 1912, Olympian, ;, football, sta...   \n",
       "2  (The, city, of, Yuma, in, this, state, has, a,...   \n",
       "3  (In, 1963, ,, live, on, \", The, Art, Linklette...   \n",
       "4  (Signer, of, the, Dec., of, Indep, ., ,, frame...   \n",
       "\n",
       "                                            combined  \n",
       "0  HISTORY For the last 8 years of his life, Gali...  \n",
       "1  ESPN's TOP 10 ALL-TIME ATHLETES No. 2: 1912 Ol...  \n",
       "2  EVERYBODY TALKS ABOUT IT... The city of Yuma i...  \n",
       "3  THE COMPANY LINE In 1963, live on \"The Art Lin...  \n",
       "4  EPITAPHS & TRIBUTES Signer of the Dec. of Inde...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df['combined'] = filtered_df[' Category'] + ' ' + filtered_df['cleaned']\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good. Let's vectorize!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column to list\n",
    "comb_list = filtered_df['combined'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold out 25% of data as a test set\n",
    "X_train, X_test = train_test_split(comb_list, test_size=0.25, random_state=26)\n",
    "#print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=10, # i want to use words that appear at least 10 times\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 19051\n"
     ]
    }
   ],
   "source": [
    "# apply the vectorizer\n",
    "comb_list_tfidf = vectorizer.fit_transform(comb_list)\n",
    "print('Number of features: %d' % comb_list_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test vectors\n",
    "X_train_tfidf, X_test_tfidf = train_test_split(comb_list_tfidf, test_size=0.25, random_state=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: EGGS & HAM The ham & cheese version of this egg dish is prepared in much the same manner as the Lorraine\n",
      "Tf_idf vector: {'manner': 0.336100988289531, 'lorraine': 0.3405620215897985, 'prepared': 0.2944894763367392, 'dish': 0.2500696568261996, 'cheese': 0.25382633746578803, 'ham': 0.6006912149220507, 'eggs': 0.26920627264605274, 'version': 0.22859153919328828, 'egg': 0.26797555090837477}\n"
     ]
    }
   ],
   "source": [
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "#print(X_train_tfidf.shape)\n",
    "#print(X_test_tfidf_csr)\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_byques = [{} for _ in range(0,n)]\n",
    "\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_byques[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_byques[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the min_df parameter to 10 words from 5 decreased the number of features from 30,000+ to 19,000. We are going to further trim the number of features using Singular Value Decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 50.3266875508026\n",
      "Component 0:\n",
      "STATE CAPITALS Chartered in 1781, it's the only state capital named for a French city                  0.393608\n",
      "WORLD GEOGRAPHY Bhopal is the capital of this country's Madhya Pradesh state                           0.392689\n",
      "THE STATE I'M IN Snowflake, Bullhead City, Scottsdale                                                  0.376095\n",
      "THE WORLD I see this city, the capital of Uruguay                                                      0.370173\n",
      "COUNTRIES OF THE WORLD It's the only country whose name is the same as an American state's             0.370134\n",
      "WORLD FACTS Cuernavaca is the capital of this North American country's state of Morelos                0.365592\n",
      "WORLD GEOGRAPHY Like the city of Bern, the Bernese Alps are in this country                            0.357448\n",
      "WORLD GEOGRAPHY Hobart is the capital city of this island state of Australia                           0.352545\n",
      "STATE CAPITAL NICKNAMES \"The First City of the First State\"                                            0.351763\n",
      "ROCHESTER Just up the turnpike from Portsmouth is the \"Lilac City\" of Rochester in this \"New\" state    0.347796\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "\n",
      "Component 1:\n",
      "EXCEPTIONAL WORDS It's a 5-letter word for \"poisonous\"                        0.664700\n",
      "3-LETTER WORDS A flipper or a fiver                                           0.655309\n",
      "9-LETTER WORDS A vortex or a jacuzzi                                          0.655309\n",
      "3-LETTER WORDS To udderly intimidate someone                                  0.655309\n",
      "6-LETTER WORDS A craven poltroon                                              0.654676\n",
      "5-LETTER WORDS The 2 5-letter words for the appliance being repaired here:    0.654631\n",
      "4-LETTER WORDS The Abominable Snowman                                         0.654282\n",
      "3-LETTER WORDS Over there; often hitched with hither                          0.654158\n",
      "3-LETTER WORDS Weep aloud, or cry uncontrollably                              0.653791\n",
      "9-LETTER WORDS It's another name for the femur                                0.653171\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "\n",
      "Component 2:\n",
      "LETTER AFTER F To abbreviate a state                                      0.730568\n",
      "5-LETTER WORDS A state of stiffness in tissue; it can precede \"mortis\"    0.702982\n",
      "4 I's ONLY A U.S. state                                                   0.699216\n",
      "THE STATE NAME ...with 2 Ks                                               0.699216\n",
      "THE STATE NAME ...with a Z                                                0.699216\n",
      "THE STATE NAME ...with the most Is                                        0.699216\n",
      "THIS & THAT This U.S. state is \"The Magnolia State\", y'all                0.698488\n",
      "U.S.A. Acadia & Evangeline are parishes in this state                     0.697351\n",
      "THE STATE I'M IN Laconia, Concord, Ossipee                                0.694464\n",
      "STATE THE OBVIOUS [State outline]                                         0.694259\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "\n",
      "Component 3:\n",
      "THE WORLD I see this city, the capital of Uruguay                                                              0.538269\n",
      "WORLD CITIES The Gaelic name for this capital city is Baile Atha Cliath                                        0.493882\n",
      "WORLD CITIES To attend Kenya Polytechnic, you have to go to this capital city                                  0.491099\n",
      "TRAVELIN' If you're in Rabat, you must be in the capital city of this country                                  0.454301\n",
      "WORLD CITIES After Rhodesia became this country, Salisbury, its capital, became Harare                         0.446536\n",
      "WORDS WITHIN WORDS This capital city could be the climax of your world tour                                    0.442798\n",
      "WORLD CAPITALS This Alpine city is the capital of a canton as well as the capital of a country, Switzerland    0.441166\n",
      "WORLD CITIES In 1788 Agha Mohammad Khan made this city the capital of Persia; it's now the capital of Iran     0.440996\n",
      "AROUND THE WORLD It's believed that Cadiz, a city in this country, was once Gadir, a Phoenician settlement     0.440692\n",
      "AROUND THE WORLD Cities in this country include Baguio, Quezon City & Manila                                   0.432057\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "\n",
      "Component 4:\n",
      "CROSSWORD CLUES \"K\" Klumsy klod              0.978194\n",
      "CROSSWORD CLUES \"F\" Falsehood                0.978194\n",
      "CROSSWORD CLUES \"C\" Have a bawl              0.978194\n",
      "CROSSWORD CLUES \"L\" Southpaw                 0.978194\n",
      "CROSSWORD CLUES \"K\" Chivalrous chesspiece    0.978194\n",
      "CROSSWORD CLUES \"A\" Ire                      0.978194\n",
      "CROSSWORD CLUES \"T\" No-no                    0.978194\n",
      "CROSSWORD CLUES \"E\" A temblor                0.978194\n",
      "CROSSWORD CLUES \"M\" Elvisopolis              0.978194\n",
      "CROSSWORD CLUES \"F\" Feydeau's forte          0.978194\n",
      "Name: 4, dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# We are going to reduce the feature space from 19,051 to 1900.\n",
    "svd= TruncatedSVD(1900)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've compressed the feature set from 19000 to 1900. This model is still able to explain 50% of the variance so that's not too bad (50% variance loss to 90% reduction in features). An attempt was made to reduce the features to 1% or 190, but that resulted in a 86% loss in variance information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the verdict?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left for us to do it apply the model to the test set with the reduced features and see if we can improve on the 55% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162697, 1900)\n"
     ]
    }
   ],
   "source": [
    "paras_by_component.head()\n",
    "print(paras_by_component.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to retrain the model with the new feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162697,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, y_test = train_test_split(filtered_df[' Round'], test_size=0.25, random_state=26)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6037603643582856\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "train = lr.fit(paras_by_component, y_train)\n",
    "train_score = lr.score(paras_by_component, y_train)\n",
    "print(train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: MOVIE STARS You may call him Rocky or Rambo, but his friends call him Sly\n",
      "Tf_idf vector: {'sly': 0.49662028909697314, 'rambo': 0.5407947504575876, 'friends': 0.35924065997827037, 'stars': 0.31674372686574803, 'rocky': 0.4063484819278554, 'movie': 0.2577009842419211}\n"
     ]
    }
   ],
   "source": [
    "#Reshapes the vectorizer output into something people can read\n",
    "X_test_tfidf_csr = X_test_tfidf.tocsr()\n",
    "#print(X_test_tfidf.shape)\n",
    "#print(X_test_tfidf_csr)\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_test_tfidf_csr.shape[0]\n",
    "\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_byques = [{} for _ in range(0,n)]\n",
    "\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_test_tfidf_csr.nonzero()):\n",
    "    tfidf_byques[i][terms[j]] = X_test_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_test[5])\n",
    "print('Tf_idf vector:', tfidf_byques[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0:\n",
      "WORLD GEOGRAPHY The largest state in Venezuela is named for this man, the country's liberator                             0.392514\n",
      "U.S. STATES It's the only state whose name & capital city both consist of 2 words                                         0.372970\n",
      "STATE CAPITALS One of the world's largest pipe organs is found in the tabernacle in this state capital                    0.367314\n",
      "SMALL STATE CAPITALS This city became a state capital in 1826, the same year the president for which it was named died    0.366973\n",
      "STATE CAPITALS This city dropped the word \"Great\" from its name in 1868, while it was still a territorial capital         0.354896\n",
      "LYING IN STATE In 1909, before reinterment, he lay in state in the city he had planned                                    0.349969\n",
      "AMERICAN WORLD CAPITALS Hey, dude! Wickenburg in this southwestern state is the \"Dude Ranch Capital of the World\"         0.342801\n",
      "STATE CAPITALS If you know that this state's capital is Jefferson City, \"show me\"                                         0.330812\n",
      "NAME THE COUNTRY \"Something is rotten in the state of\" this                                                               0.330555\n",
      "STATE CAPITALS If you drove to this state capital, you'd find it's an anagram of one of the words in this answer          0.329831\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "\n",
      "Component 1:\n",
      "4-LETTER WORDS Bacall's last word on how to whistle                        0.670119\n",
      "3-LETTER WORDS 3-letter word often partnered with neither in a sentence    0.658095\n",
      "3-LETTER WORDS A cackleberry                                               0.655303\n",
      "7-LETTER WORDS Not one or the other                                        0.655303\n",
      "4-LETTER WORDS To descend to the bottom of anything, even a lavatory       0.654942\n",
      "4-LETTER WORDS A perpetual one was tended to by the Vestal Virgins         0.654407\n",
      "3-LETTER WORDS Quixote or Drysdale                                         0.653900\n",
      "\"F\"IVE-LETTER WORDS The thighbone                                          0.652814\n",
      "3-LETTER WORDS Unnaturally pale... two... three                            0.652277\n",
      "4-LETTER WORDS It can be a custardy tart or a creme caramel                0.649876\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "\n",
      "Component 2:\n",
      "THE STATE NAME ...with the most Os                                                                                  0.696174\n",
      "STATE THE OBVIOUS [State outline]                                                                                   0.694410\n",
      "WHAT THE \"H\" IS IT? Das kapital of das Keystone State                                                               0.681259\n",
      "STATE INTELLIGENCE Komitet Gosudarstvennoy Bezopasnosti                                                             0.679493\n",
      "LYING IN STATE Bonnie & Clyde                                                                                       0.677640\n",
      "SCIENC\"E\" In thermodynamics, a closed system slowly evolves towards a state of maximum this, inactivity             0.642251\n",
      "STATE CAPITALS If you drove to this state capital, you'd find it's an anagram of one of the words in this answer    0.633145\n",
      "THE STATE I'M IN Pottsville, Schuylkill Haven, Altoona                                                              0.629863\n",
      "SAME FIRST & LAST LETTER A 4-letter U.S. state                                                                      0.609901\n",
      "UNIVERSITY STATE Grambling State, Dillard, Tulane                                                                   0.591138\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "\n",
      "Component 3:\n",
      "I'M INTO WORLD \"P\"s We are marching to the former capital of Transvaal, this city north of Johannesburg                     0.444965\n",
      "DIRTY 4-LETTER WORDS It's a polluted haze found over a city                                                                 0.435865\n",
      "OMAN This, the capital city, was formerly paired with Oman in the country's name                                            0.433983\n",
      "AROUND THE WORLD Bandung, this country's 3rd-largest city, is the capital of the province of West Java                      0.421650\n",
      "IN COUNTRY Uranium City, Inuvik, Yellowknife                                                                                0.413886\n",
      "WORLD CITIES First settled by Germanic & Slavic tribes, this city served as the capital of Brandenburg from 1486 to 1701    0.411890\n",
      "WORLD CITIES Pakistan's largest city, it was the country's capital from 1947 to 1959                                        0.407777\n",
      "WORLD CAPITALS One of this city's top attractions is the masuoleum of Kemal Ataturk                                         0.407281\n",
      "WORLD CITIES This Swiss city is headquarters to the World Council of Churches                                               0.405854\n",
      "POPCORN This country produces most of the popcorn consumed in the world                                                     0.403093\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "\n",
      "Component 4:\n",
      "CROSSWORD CLUES \"B\" Oleo is its understudy    0.97819\n",
      "CROSSWORD CLUES \"F\" 3/4 or 2/5                0.97819\n",
      "CROSSWORD CLUES \"L\" Serendipity               0.97819\n",
      "CROSSWORD CLUES \"V\" Foxlike                   0.97819\n",
      "CROSSWORD CLUES \"T\" Hatchets & hacksaws       0.97819\n",
      "CROSSWORD CLUES \"M\" Clemency                  0.97819\n",
      "CROSSWORD CLUES \"A\" So be it                  0.97819\n",
      "CROSSWORD CLUES \"M\" Macaques & marmosets      0.97819\n",
      "CROSSWORD CLUES \"M\" Perhaps                   0.97819\n",
      "CROSSWORD CLUES \"P\" To Xerox                  0.97793\n",
      "Name: 4, dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# applying lsa model to test set\n",
    "X_test_lsa = lsa.transform(X_test_tfidf)\n",
    "\n",
    "test_ques_by_component=pd.DataFrame(X_test_lsa,index=X_test)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(test_ques_by_component.loc[:,i].sort_values(ascending=False)[0:10])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  0.5850128150756919\n",
      "[[15211     1 11201     0]\n",
      " [  583     3   351     0]\n",
      " [10367     1 16513     0]\n",
      " [    1     0     1     0]]\n",
      "Jeopardy!           26881\n",
      "Double Jeopardy!    26413\n",
      "Final Jeopardy!       937\n",
      "Tiebreaker              2\n",
      "Name:  Round, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# now let's apply the logistic regression  to the test and make some predictions\n",
    "test_score = lr.score(test_ques_by_component, y_test)\n",
    "print('Test score: ', test_score)\n",
    "\n",
    "y_pred = lr.predict(test_ques_by_component)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "58.5%! That's a 3 point improvement over the model fit with just the question. It's a small improvement but an improvement nonetheless. Looking at the confusion matrix, this model just isn't very good at predicting Final Jeopardy questions. My hypothesis is that the difficulty of a question either cannot be predicted by the frequency of a term, or the length of each question with it's limited length is just difficult to extract meaningful relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
